# Likelihood estimation and the Box--Jenkins method



This tutorial addresses the following:

- estimation of ARIMA and ARCH models using conditional maximum likelihood.
- the Box--Jenkins methodology for selecting the order of ARMA processes based on analysis of the (partial) correlogram.
- model selection using information criterion, unit roots and problems arising from model fit.


## Manual maximum likelihood estimation

As was done in class for the `beaver` dataset, we will look at manual specification of the likelihood. While it is straightforward in principle to maximize the latter for ARMA models, the numerous restrictions that are imposed on the parameters make it hard, if not impossible, to manually code one's own function. Maximum likelihood estimation is implemented typically via the state-space representation, which we will cover later in the semester.

For simple models, it is easily done however, and should shed some light on the various functions that are part of **R** for optimization, the definition of a function, the use of `nlm` and `optim` for optimization purposes, etc.

We first load a dataset of UBS and Credit Suisse stock prices from 2000 until 2008. The data is splitted in three parts for the analysis, since the data is heteroscedastic, and there appears (visually) to be two changepoints. We look at the adequacy of fitted AR(1) model for the mean and an ARCH(1) for the variance.

```{r ubs_series}

#devtools::install_github("nickpoison/astsa")
#devtools::install_github("joshuaulrich/xts")
library(xts); library(lubridate)
# read data and examine it
UBSCreditSuisse <- read.csv("http://sma.epfl.ch/~lbelzile/math342/UBSCSG.csv", stringsAsFactors = FALSE)
names(UBSCreditSuisse)
head(UBSCreditSuisse)

# create time series, accounting for missing values at weekends and 251.25 values/year
# this is correct for analysis, but only provides approximate locations for plotting
UBS <- ts(UBSCreditSuisse$UBS_LAST, start = c(2000, 1), frequency = 365.25) 
UBS <- ts(UBS[!is.na(UBS)], start = c(2000, 1), frequency = 251.625) 

#Irregular time series
UBS_xts <- with(UBSCreditSuisse, xts(UBS_LAST, mdy(Date)))
```

Objects of class `ts` store the dates from the vector start with observations as $(i-1)/\omega$. Thus, we specified in the above a vector encoded as 2000, 2000+1/365.25, \ldots This means that missing values are not handled. In contrast, `xts` objects keep the time stamps from a `Date` object. The function `with` is equivalent to `attach`, but has a limited scope and is used to avoid writing `UBSCreditSuisse$Date`, etc. The function `mdy` transforms the string `Date` as month, day and year. The string is coerced into an object of class `Date`.

```{r}
# Analysis for UBS returns, 2000-2008
UBS_ret <- 100*diff(log(UBS_xts))
plot.zoo(UBS_ret, xlab = "Time", ylab = (ylab <- "Daily returns (in %)"), main = "Percentage daily growth rate of UBS stock")

# compute log returns
UBS.ret <- 100*diff(log(UBS))

# split into 3 homogeneous(?) parts, and plot using the same vertical axis on the graphs

#with the xts object
par(mfrow = c(1, 3))
lims <- range(UBS.ret)
plot.zoo(UBS_ret[paste0(index(first(UBS_ret)), "/", as.Date("2003-01-01")+100)], ylim = lims, 
         xlab = "Time", ylab = ylab)

#with window and the ts object
y1 <- window(UBS.ret, end = c(2003, 100))
#plot(y1, ylim = lims)

y2 <- window(UBS.ret, start = c(2003, 101), end = c(2007, 200))
plot(y2, ylim = lims, ylab = ylab, main = "UBS daily growth rate (in %)")

y3 <- window(UBS.ret, start = c(2007, 201))
plot(y3, ylim = lims, ylab = ylab)

# analysis of first part, first just plotting ACF and PACF for data and for abs(data)
y <- y1

#(Partial) correlograms for the series
par(mfrow = c(2, 2))
TSA::acf(y, lag.max = 100, main = "Daily log returns (%)")
pacf(y, lag.max = 100, , main = "")
TSA::acf(abs(y), lag.max = 100, main = "Absolute daily log returns (%)")
pacf(abs(y), lag.max = 100, main = "")
```

The residuals look pretty much white noise, but the variance has residual structure. Recall the implicit definition of the AR(1) process $Y_t$,  \[Y_t=\mu+\phi(Y_{t-1}-\mu)+\varepsilon_t,\]
where $\varepsilon_t \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(0,\sigma^2)$. The joint distribution of the observations conditional on the first is multivariate normal. Here is a simple function for the likelihood, which only requires specifying the conditional mean.

```{r ar_arch fit}
# analysis using AR(1) model for means
# conditional likelihood
nll_AR1 <- function(th, y) {
	n <- length(y)
	condit.mean <- th[1]+th[3]*(y[-n]-th[1])
	-sum(dnorm(y[-1], mean = condit.mean, sd = th[2], log = TRUE)) 
}
init1 <- c(0, 1, 0.5)
#fit1 <- nlm(f = nll_AR1, p = init1, iterlim = 500, hessian = TRUE, y = y)
fit1 <- optim(init1, nll_AR1, y = y, hessian = TRUE, method = "Nelder-Mead")
```


We obtain the parameter estimates and the standard errors from the observed information matrix, estimated numerically. Incidently, one can easily that the problem is equivalent to a linear Gaussian model where the regressor is a lagged vector of observations. The parameter estimates differ slightly, but this is due to the optimization routine.


```{ar1_param}
#Parameter values (MLEs)
fit1$par
#Standard errors from inverse of Hessian matrix at MLE
#If you code the optimization routine yourself, you can still obtain the Hessian via
#hessian <- numDeriv::hessian(func = nll_AR1, y = y, x = fit1$par)

#Standard errors
sqrt(diag(solve(fit1$hessian)))

#Conditional likelihood using lm 
#dynlm is a wrapper around lm for `ts` and `zoo` objects, L means lag and you can add e.g. trend(y)
fit1_ols <- dynlm::dynlm(y ~ L(y, 1))
coefficients(fit1_ols)
sd(residuals(fit1_ols))
```

Incidently, the situation is analogous for the ARCH(1) process, which has a conditional variance that changes over time. The latter is defined implicitly as
\begin{align*}
Z_t &= \mu + \sigma_t\epsilon_t\\
\sigma_t^2 &=\alpha_0+\alpha_1(Z_{t-1}-\mu)^2 
\end{align*}
with $\epsilon_t \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(0, 1)$.

The variance $\sigma^2$ here is included as $\sigma^2=\alpha_0$ and the parameter appearing in the likelihood is $\alpha_1/\sigma^2$ corresponds to $\theta_3$, or `th[3]`.

```{r arch1_nll}
# analysis using ARCH(1) model for variances
nll_ARCH1 <- function(th, y) {
	n <- length(y)
	condit.mean <- th[1]
	condit.var <- th[2]*(1+th[3]*(y[-n]-th[1])^2)
	-sum(dnorm(y[-1], mean = condit.mean, sd = sqrt(condit.var), log = TRUE)) }
init2 <- c(0, 1, 0.5)
fit2 <- nlm(f = nll_ARCH1, p = init2, iterlim = 500, hessian = TRUE, y = y)
## fit2 <- optim(init2, nll_ARCH1, y = y, hessian = TRUE)
```

The function `nlm` performs minimization, but may return warnings because some of its steps because the conditional variance can be negative for some combinations of the variable, so the corresponding moves of the Newton algorithm are rejected. These are typically steps that are not in the neighborhood of the final solution, so can be ignored if the output is valid. The `minimum` corresponds to the negative log-likelihood at the maximum likelihood estimates. The MLE is given by `estimate` and the standard errors by the square root of the diagonal entries of the inverse Hessian (here already negated because we work with the negative of the log-likelihood). Since the residuals have a varying variance, we need to adjust them by dividing each by their respective variance. Same would have occured for the AR(1) process, but it is easier for the mean.

```{r arch1_coefs}
fit2$minimum
fit2$estimate
sqrt(diag(solve(fit2$hessian)))

make_resid_ARCH1 <- function(y, fit){
  th <- fit$estimate
	n <- length(y)
	condit.mean <- th[1]
	condit.var <- th[2]*(1+th[3]*(y[-n]-th[1])^2)
    res <- (y[-1]-condit.mean)/sqrt(condit.var)
    ts(res)
}

res2 <- make_resid_ARCH1(y, fit2)

```

We now proceed with diagnostic plots to check the model adequacy. Recall that the Kolmogorov--Smirnov test statistic associated with
the cumulative periodogram tests the hypothesis of white noise (and ARCH(1) is white noise).

```{r diagnostics_ARCH}
par(mfrow = c(2, 2))
plot(y, main = "Raw returns")
plot(res2, main = "ARCH(1) residuals")
cpgram(res2, main = "Cumulative periodogram")
par(pty = "s")
qqnorm(res2, panel.first = { abline(0, 1, col = "red")})
```


### Exercise 1: UBS stock returns

1. Create a function that fits an AR(1)-ARCH(1) model by modifying the code provided above and apply it to `y`. The latter is defined as
\[Y_t-\mu = \phi(Y_{t-1}-\mu)+\sigma_t\varepsilon_t, \quad \sigma^2_t = \alpha_0+\alpha_1(Y_{t-1}-\mu)^2, \quad \varepsilon_t \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(0,\sigma^2)\]
2. Obtain the maximum likelihood estimates using `nlm` or `optim` as well as the standard errors
3. Plot the residuals. Comment on the fit using standard diagnostic plots (Q-Q plot, ((P)ACF, cumulative periodogram).
4. Fit an AR(2) model using a conditional likelihood for the mean and obtain the standard errors of your estimated coefficients.
5. Perform a likelihood ratio test to test whether the AR(2) coefficient is significative.


## Box--Jenkins methodology for ARMA models


The Wold decomposition theorem states that any second-order stationary time series can be represented as a deterministic process and a stochastic linear process, which can be represented as a  causal MA($\infty$) series of the form
\[Y_t = \sum_{j = 0}^\infty \psi_j\varepsilon_{t-j}, \qquad \varepsilon_t \sim \mathrm{WN}(0, \sigma^2),\quad \psi_0 = 1, \quad\sum_{j = 0}^\infty \psi_j^2 < \infty\]

This does provide a justification for using an ARMA model for modelling. The latter may not be parsimonious nor useful at characterizing the data generating mechanism, but they nevertheless provide in some simple cases a good approximation.

The Box--Jenkins methodology for ARMA models (dating back to time where computing ressources were scarce) allows one to select the order of an AR($p$), MA($q$) or ARMA($p, q$) by visual inspection of the (partial) correlograms. Both should always go alongside one another.

1. Apply a transformation of the data $X_t$ where appropriate 
  - logarithm, Box--Cox transform
  - differencing so that the series appears linear.
2. Correlogram
  - Determine the MA($q$) order by looking at the autocorrelation, at the points for which $\rho_k \neq 0$ for $k \leq q$ and $r_k \approx 0$ for $k>q$. 
  - For an AR($p$) process, the autocorrelation function should decay exponentially, with possible oscillation patterns.
  - For an ARMA($p, q$) model, the pattern is irregular for lags $k = 1, \ldots, p$ and go to zero as $k \to \infty$. 
  
3. Partial correlogram
  - Parameters should be zero at lags $k>p$ for the AR($p$) model, and nonzero otherwise
  - The parameters decay exponentially in the MA($q$) model
  - The parameters decrease to zero as $k \to \infty$ for the ARMA($p, q$) model.


The function to fit these models is `arima`, whose arguments are specified via `order = c(p, d, q)`. Ignore the `d` component for now in the triple $(p, d, q)$ by setting it to zero. Other options are `sarima` from `astsa`, which is a wrapper around `arima`. `sarima` provides diagnostic plots alongside and includes a constant by default, but the syntax differs from `arima` and it takes directly components $p$, $d$ and $q$. The function `Arima` from Hyndman's `forecast` package is yet another wrapper around `arima`. An explanation of the differences can be found in [Forecasting: principles and practice](https://www.otexts.org/fpp/8/7), at the bottom of the page. 
  

> The `Arima()` command from the `forecast` package provides more flexibility on the inclusion of a constant 

It also correctly labels the latter. Depending on your model, it may be the level (mean), an intercept, the linear trend (slope, or drift in the time serie literature). If we take first difference, the constant is the drift, etc.

**Warning**: You may stumble on the web on `auto.arima.` Beware of the naive and automated selection implemented by this function (which relies on what I would consider to be an *ad hoc* forward model selection). Use at your own risk.


ARMA models can be almost equivalent, as the following example from Shumway and Stoffer (example 3.28) illustrates. Note that as we use `sarima`, a constant is included by default. We can assess its significance by the usual $t$-test, if the error structure is appropriate.

```{r}
library(astsa)
gnpgr = diff(log(gnp)) # growth rate of GNP
main <- "Quarterly growth rate\n of U.S. GNP"
plot(gnpgr, main = main, ylab = "Growth rate")
#There is a different mean in each quarter, but forego the seasonal effect
#This is obvious in the following plot, which plots (in order), separating by quarter
#monthplot(gnpgr, main = main, ylab = "Growth rate", xlab = "Quarter")
par(mfrow = c(1, 2))
TSA::acf(gnpgr, 24, main = main); pacf(gnpgr, 24, main = main)
#What does the period in the correlogram correspond to?

```

The decrease in ACF/PACF suggests that either an AR(1) or an MA(2) might be appropriate here.

```{r shumway_example}
mod1 <- sarima(gnpgr, p = 1, d = 0, q = 0, details = FALSE) # AR(1)
print(mod1$ttable)
mod2 <- sarima(gnpgr, p = 0, d = 0, q = 2, details = FALSE) # MA(2)
print(mod2$ttable)
```

Having fitted these two models, we compare them using their linear proces (or MA($\infty$) representation) and the theoretical autocorrelation and partial autocorrelation coefficients.

```{r gnp_ar1orma2_comparison}
#Obtain the coefficients from the MA(Inf)
ARMAtoMA(ar = mod1$fit$coef[1], ma = 0, 4) # prints first 4 psi-weights
#Print sample ACF and superpose the theoretical coefficients implied by the model
par(mfrow = c(1, 2))
TSA::acf(gnpgr, 10, main = main)
points(seq(0, by = 0.25, length = 11), y = ARMAacf(ar = mod1$fit$coef[1], lag.max = 10), col = "blue")
points(seq(0, by = 0.25, length = 11), y = ARMAacf(ma = mod2$fit$coef[1:2], lag.max = 10), col = "red")

pacf(gnpgr, 10, main = main)
points(seq(0.25, by = 0.25, length = 10), y = ARMAacf(ar = mod1$fit$coef[1], lag.max = 10, pacf = TRUE), col = "blue")
points(seq(0.25, by = 0.25, length = 10), y = ARMAacf(ma = mod2$fit$coef[1:2], lag.max = 10, pacf = TRUE), col = "red")
```

**Warning**: in the example above, it is necessary here to add a constant coefficient. Economic theory suggest exponential growth, hence a trend of $\exp(t \beta)$. This becomes a linear trend $t\beta$ for log returns, and by differencing, we obtain the trend $\beta$. The latter corresponds to the long term trend, or the quarterly growth rate, which is about 1%. The coefficient is significant even if it seems small and failing to include it leads to invalid inference about the state of the U.S. economy.

The last step is to check whether our process is causal and invertible. For the former, `arima` typically transforms to the stationary and invertible solution, so we should be good. We may end up having a root of $\Phi(B)$ or $\Theta(B)$ on the unit circle. In this case, the ARMA process is not stationary, while in the second, asymptotic normality of the estimator breaks down. We look at the roots of the polynomial $\Phi(B)$ and $\Theta(B)$ and check they indeed are outside the unit circle. It is easiest to check their norm or modulus.

```{r shumway_causalinvert}
#Is the MA(2) process invertible?
abs(polyroot(c(1, mod2$fit$coef[1:2]))) #MA roots
#Is the AR(1) process causal
Mod(polyroot(c(1, -mod1$fit$coef[1]))) #AR roots
```

Sometimes models give similar output. The model choice should then be made on the grounds of **parsimony**: a simpler model that explains the data should be prefered. If the models are nested, for example an AR(1) and an AR(2), then a likelihood ratio test can be performed (watch out to make sure the same number of observations appear in the models if using a conditional likelihood). A quick way to do this is by looking at the Akaike's information criterion. There are other information criterion, notably the BIC and bias-corrected version of AIC termed $\mathrm{AIC}_{\mathrm{c}}$. They are defined respectively as

\begin{align*}
\mathrm{AIC}& = -2\ell(\boldsymbol{\theta})+2k,\\
\mathrm{BIC}& = -2\ell(\boldsymbol{\theta})+k\log(n),\\
\mathrm{AIC}_{\mathrm{c}}& = -2\ell(\boldsymbol{\theta})+2 \frac{kn}{n-k-1}.
\end{align*}

In the formulas above, $n$ is the sample size, $k$ is the number of parameters in the model and $\ell$ is log-likelihood. Since likelihood values increase with the number of parameters provided the models are nested (why?), the additional term penalizes complex models. The Schwartz's information criterion (or Bayesian information criterion, BIC in short) includes the sample size in the penalty. The penalty could be viewed as a testing procedure similar to likelihood ratio test for nested models, where in place of the quantiles of the $\chi^2$ distribution, one uses e.g. for AIC a threshold of $2(k_1-k_2)$.

The BIC is consistent, meaning that it chooses the correct model with probability 1 as $n \to \infty$. AIC is *not* consistent for model selection and this results typically in overfitting. For ARMA models models with zero mean, Brockwell and Davis advocate the use of a small-sample correction of AIC, AIC$_\mathrm{c}$, with $k = p+q+1$. The latter is equivalent to AIC when $n \to \infty$.

**Warning**: The information criterion returned by `astsa::sarima` does not correspond to the definition above; the one implemented can be found in Shumway and Stoffer's book.  The principle remains the same: the lower, the better.

Here is an illustrative example on the dataset in the following exercise with a helper function to find the roots of the polynomials. I would like to bring your attention to the fact about the difficulties of optimizing some time series models. For example, one often finds that the ARMA(2,1) fits as well as the AR(1), even when the latter is the true underlying model. However, a quick look at the output below is illustrative of the trouble that lies ahead.

In the next exercise, you will be asked to practice your skills on simulated datasets. Before letting you proceed, I want to illustrate the different commands for fitting e.g. an ARMA(2, 1) model, and see what comes out.

```{r charlotte_examples}
library(forecast); library(astsa)
load(url("http://sma.epfl.ch/~lbelzile/math342/Simulated_ARMA.RData"))

#Wrapper around arima from forecast library
mod_a <- arima(x3, order = c(2, 0, 1), transform.pars = TRUE) 
mod_b <- Arima(x3, order = c(2, 0, 1), transform.pars = TRUE)   
#Arima returns BIC, AIC and AICC
mod_c <- sarima(x3, p = 2, d = 0, q = 1, details = FALSE)
```
```{r echo=FALSE}
invisible(capture.output(sarima(x3, p = 2, d = 0, q = 1)))
```
```{r}
#sarima provides diagnostic plots unless `details=FALSE`
#unfortunately also returns all the optimization steps from `arima` in the new version
mod_a
mod_b
mod_c
```

You can (and should) check the roots of the AR and MA polynomials to make sure this is not happening. The argument `transform.pars` transforms the AR polynomial to the causal representation if it does not have a root on the unit circle. If the process is not invertible, the parameter lies on the boundary of the space and the usual standard errors obtained from the observed information matrix are not reliable. It is possible to use these models for forecasting, but their interpretation is awkward.


### Exercise 2: Simulated series
1. The `Simulated_ARMA` dataset contains 8 time series (these examples are from Charlotte Wickham's website). Fit ARMA models to those and select a model, justifying your choice. Be skeptical of the optimization routine and analyze the models carefully.



## Information criteria, model selection and profile likelihood

The following example is due to [Edward Ionides](http://ionides.github.io/531w16/notes05/notes5.html) and is licensed under CC-BY-NC.

We analyze data from NOAA, the depth of Lake Huron (in m). There is a decreasing trend. We first format the time variable and extract the height in January.

```{r lake_huronf}
library(lubridate)
#Data from http://www.glerl.noaa.gov/data/dashboard/data/levels/mGauge/miHuronMog.csv
dat <- read.csv("http://sma.epfl.ch/~lbelzile/math342/huron_depth.csv", sep = ",", header = TRUE, skip = 2)
dat$Date <- strptime(dat$Date, "%m/%d/%Y")
dat$year <- with(dat, year(Date))
dat$month <- with(dat, month(Date)) #as.numeric(format(dat$Date, format = "%m"))
head(dat)

## Subsample values to take only January data 
dat <- subset(dat, month == 1)
huron_depth <- dat$Average
year <- dat$year
#Plot the series
plot(huron_depth~year, type = "l", ylab = "Lake depth (m)", main = "Monthly Average Master Gauge\nWater Levels (1860-Present)")
```

Next, we create a table containing the AIC values, which are obtained from fitting successive ARMA models with varying orders $p$, $q$.
Here, a simple AR(1) does an excellent job at capturing the structure and all other models are essentially more complicated versions.

```{r}
huron_ar1 <- Arima(huron_depth, order = c(1, 0, 0), include.drift = TRUE, include.mean = TRUE)

## Table of AIC values extracted from the output - notice the warnings on your computer when the fit fails!
aic_table <- function(data, P, Q){
  table <- matrix(NA, (P+1), (Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
       table[p+1, q+1] <- arima(data, order = c(p, 0, q))$aic
    }
  }
  dimnames(table) <- list(paste("<b> AR", 0:P, "</b>", sep = ""), paste("MA", 0:Q, sep = ""))
  table
}
huron_aic_table <- aic_table(huron_depth, 4, 5)
require(knitr)
kable(huron_aic_table, digits = 2)

##Kables are tables for HTML. For LaTeX, use something like
# library(xtable)
# dimnames(huron_aic_table) <- list(paste0("AR", 0:4), paste0("MA", 0:5))
# latex_tab <- xtable::xtable(huron_aic_table, booktabs = TRUE, 
#                           caption = "AIC values for ARMA models for the Lake Huron dataset")
# print(latex_tab, booktabs = TRUE, caption.placement = "top")
```

I silenced the output, but some of the optimization routine *failed* (duh). You can see this by looking at the AIC values (recall the definition and look at nested models). These cannot increase by more than 2 from left to right or top to bottom.

Subsequent quotes are from Prof. Ionides' notes

> What do we learn by interpreting the results in the above table of AIC values? In what ways might we have to be careful not to over-interpret the results of this table?

We can look at the roots to see if the process of our choice (the one with lowest AIC, say) is good. Overly complicated models can lead you to big troubles. We illustrate this by fitting a complex model, an ARMA(2,1), to the series:

```{r fit_arma21}
## Fit an ARMA(2, 1) model
huron_arma21 <- Arima(huron_depth, order = c(2, 0, 1), include.drift = TRUE, include.mean = TRUE)
huron_arma21

## Root of the Phi polynomial
AR_roots <- polyroot(c(1, -coef(huron_arma21)[grep("^ar", names(huron_arma21$coef))]))
Mod(AR_roots)
```

The process is causal, but the estimate of the MA coefficient, $\theta_1$ is on the boundary of the parameter space and numerically indistinguishable from 1.

> Let’s investigate a little, using profile methods. The claimed standard error on the MA(1) coefficient, from the Fisher information approach used by `arima` is small. We can see if the approximate confidence interval constructed using profile likelihood is in agreement with the approximate confidence interval constructed using the observed Fisher information.

> Note that `arima` transforms the model to invertibility. Thus, the estimated value of  $\theta_1$ can only fall in the interval $(−1, 1)$ but can be arbitrarily close to $−1$ or $1$.

Recall the definition of profile likelihood: for a parameter of interest $\boldsymbol{\psi}$ and a partition $\boldsymbol{\theta}=(\boldsymbol{\psi}, \boldsymbol{\lambda})$, the profile likelihood as a function of  $\boldsymbol{\psi}$ is
\[\ell_p(\boldsymbol{\psi}) = \mathrm{argmax}_{\boldsymbol{\lambda} \in \Lambda} \ell(\boldsymbol{\lambda} \mid \boldsymbol{\psi}).\]

The maximum profile likelihood coincides with the maximum likelihood estimate (why?), and we use it normally to obtain more accurate confidence intervals for (a) parameter(s) of interest. Here, we investigate the profile log-likelihood of the MA(1) coefficient. Unlike the nice examples you saw in class, the profile here is weird and the global maximum lies on the boundary. 

```{r profile_confint}
## Profile log-likelihood for the MA coefficient
K <- 1000
ma1 <- seq(from = -0.99, to = 1, length = K)
profile_loglik <- rep(NA, K)
for(k in 1:K){
   profile_loglik[k] <- logLik(arima(huron_depth, order = c(2, 0, 1),  xreg= scale(seq(1:length(huron_depth))), include.mean = TRUE, 
      fixed = c(NA, NA, ma1[k], NA, NA)))
}
#Failed for some values of the MA 
plot(profile_loglik-logLik(huron_arma21)~ma1, type = "l", 
     ylab = "Profile log likelihood", xlab = "value", 
     main = expression(paste("Profile log-likelihood of ", theta[1], " for the ARMA(2, 1) model")))
abline(h=-qchisq(0.95,1)/2, col=2, lty=2)
abline(v=1, col=1)
```

The "usual" 95% profile confidence interval would thus include zero, so the coefficient is maybe not significant after all. I coded a little function that you can use to find the value at which a profile log-likelihood, here shifted so that the maximum is at zero, intersects with the value of half the $\chi^2$ quantile.

```{r}
#mle <- logLik(huron_arma21)[1]
profile_confint <- function(mle, xvals, profile, lev = 0.95){
  K <- length(profile)
  ind_max <- which.max(profile) #value corresponding to MLE
  val_conf <- mle - qchisq(lev, 1)/2 #Cutoff line
  
  #Find the closest value by linear interpolation, swapping x and y
  upper <- profile[(ind_max+1):K]
  lower <- profile[1:ind_max]
  up <- suppressWarnings(c(max(which(upper >=  val_conf)), 
                           min(which(upper < val_conf))) + ind_max)
  low <- suppressWarnings(c(min(which(lower >=  val_conf)), 
                            max(which(lower < val_conf))))
  #Linear interpolation step with two closest values
  if(!any(is.infinite(low))){
  a <- approx(y = ma1[low], x = profile_loglik[low], xout = val_conf[1])$y
  } else{
   warning("No value of the profile likelihood is below the threshold
           for the lower confidence interval")
    a <- NA
  }
  if(!any(is.infinite(up))){
  b <- approx(y = ma1[up], x = profile_loglik[up], xout = val_conf[1])$y
  } else{
   warning("No value of the profile likelihood is below the threshold
           for the upper confidence interval")
    b <- NA
  }
  return(c(a, b))
}
#Profile confidence interval 
profile_confint(logLik(huron_arma21)[1], xvals = ma1, profile = profile_loglik, lev = 0.95)

```

Here, the "usual" 95% confidence interval includes (because of the constraint) all values in `r paste0("(", round(profile_confint(logLik(huron_arma21)[1], xvals = ma1, profile = profile_loglik, lev = 0.95)[1], 2),", 1)")`.


> - What do you conclude about the Fisher information confidence interval proposed by `arima`?
 - When do you think the Fisher information confidence interval may be reliable?
 - Is this profile likelihood plot, and its statistical interpretation, reliable? How do you support your opinion on this?


Be suspicious of the output and think before acting.

There is in fact a non-zero probability of obtaining a coefficient of $\pm 1$ for an MA(1). This can be easily seen through a simulation. We see a picture that roughly matches that of the profile likelihood (noting what seems to be convergence to a local maximum at $\theta=-1$ in many cases).

```{r madist_sim, cache=TRUE}
set.seed(54321)
hist(MAcoefdist <- replicate(n=1000, #perform 1000 replicates
      coef(Arima(order=c(2,0,1),  #extract ma[1] coef from ARMA(2,1)
arima.sim(n=155, model=list(ar=coef(huron_ar1)["ar1"])),))["ma1"]), #data is 155 obs from AR(1) model
breaks=300, main="Estimate of the MA(1) coefficient\n in an ARMA(1,2) model", xlab="MA(1) coefficient value")
#Proportion of values that lie on the boundary.
mean(I(abs(MAcoefdist)>0.999))
```

We wish to restrict to causal and invertible processes. The latter implies that the AR($\infty$) representation of the process exists; the latter is used for prediction. However, while unit roots happen rarely, you only get one sample and it may happen in yours. This makes life more complicated because the asymptotics are non-regular for the problem. It is thus best to rely on procedures that only require estimation of the model under the null hypothesis, such as a score test.


### Exercise 3: Lake Erie height

1. Perform an additive decomposition of the form 
\[Y_t = m_t + s_t + Z_t\] where $m_t$ is the trend and $s_t$ is a seasonal component of the Lake Erie height series, found at [http://sma.epfl.ch/~lbelzile/math342/LakeErie.csv]([http://sma.epfl.ch/~lbelzile/math342/LakeErie.csv]). Characterize the stochastic component $Z_t$ using an ARMA model.
2. Obtain a table of AIC and BIC values for ARMA($p, q$) model for order 7 and less. Anything worth of notice?
3. Justifying your answer, select an adequate ARMA model. Are the residuals from this model white noise? 
4. Look at the fitted model and check for invertibility and causality of your solution.
5. Plot the (partial) correlogram of $Z_t$ and superpose the theoretical coefficients implied by your model.
6. Plot the correlogram of the residuals of your ARMA model. Do they appear white noise?
7. Perform a Ljung-Box test on the residuals. What can you conclude?

